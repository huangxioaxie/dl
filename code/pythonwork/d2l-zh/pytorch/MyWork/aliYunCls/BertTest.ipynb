{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523c7ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb638998e7349389fe4feb78e0b6763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f776996d25433fba39612c88ceec31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d334c136a9aa426bbc65570e57ce3b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079e9692784e405dbf96e5889142c3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'really', 'enjoyed', 'this', 'movie', 'a', 'lot', '.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence='I really enjoyed this movie a lot.'\n",
    "#1.Tokenize the sequence:\n",
    "tokens=tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd6b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d642ae3da64bdca2531f6ed55d2f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07dda53baf34b58aec731c4ea6d0bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b9562de1d24c7b96bb1bffae135530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f6e93bc9e644d0b422b369cf4b3f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very happy to show you the ğŸ¤— Transformers library.')\n",
    "# [{'label': 'POSITIVE', 'score': 0.9998}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b33c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8613640069961548}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('æˆ‘çˆ±åŒ—äº¬')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4423c4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.5718990564346313}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('æ™®äº¬è¿›æ”»ä¹Œå…‹å…°')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f33ded9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb90308f91b43e589faf57b93d5c0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c4fb7ec6784255a2802d6d4b0fd6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c439bc861fb54175ab8e085d6b0eb699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc61639d4ef41269c84262596a2ebe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"A Titan RTX has 24GB of VRAM\"\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "print(tokenized_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f79481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'ï¼Œ', '[UNK]', 'ä¸€', '[UNK]', '[UNK]', '[UNK]', 'ï¼Œ', '[UNK]', 'æ–‡', 'æœ¬', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"è‡ªç„¶å¤„ç†ä»»åŠ¡æ€»ä½“å¯ä»¥åˆ†ä¸ºä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥åˆ†è¯ï¼Œå°†æ–‡æœ¬é€š\"\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "print(tokenized_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68262409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1806e78374464254969faa23eba493d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff083190286545b79ae0af7b6900cf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cf9b26b921426593381d90ff5221df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db44d287c7a4b80a01c03e70c20d5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    " \n",
    "model_name = 'bert-base-cased'   # ä¸‹è½½çš„æ•°æ®æ¨¡å‹åç§°\n",
    "cache_dir = './cache_dir'        # å°†æ•°æ®ä¿å­˜åˆ°çš„æœ¬åœ°ä½ç½®\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "# ä½¿ç”¨cache_dir å¯ä»¥æŒ‡å®šæ–‡ä»¶ä¸‹è½½ä½ç½®\n",
    " \n",
    "sequence = \"A Titan RTX has 24GB of VRAM\"  # å¾…åˆ†è¯çš„å¥å­\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)  # åˆ†è¯\n",
    "print(tokenized_sequence)  # è¾“å‡º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44999192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101,\n",
      "               138,\n",
      "               18696,\n",
      "               155,\n",
      "               1942,\n",
      "               3190,\n",
      "               1144,\n",
      "               1572,\n",
      "               13745,\n",
      "               1104,\n",
      "               159,\n",
      "               9664,\n",
      "               2107,\n",
      "               102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from pprint import pprint\n",
    " \n",
    "model_name = 'bert-base-cased'   # ä¸‹è½½çš„æ•°æ®æ¨¡å‹åç§°\n",
    "cache_dir = './cache_dir'        # å°†æ•°æ®ä¿å­˜åˆ°çš„æœ¬åœ°ä½ç½®\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "# ä½¿ç”¨cache_dir å¯ä»¥æŒ‡å®šæ–‡ä»¶ä¸‹è½½ä½ç½®\n",
    "sequence = \"A Titan RTX has 24GB of VRAM\"  # å¾…åˆ†è¯çš„å¥å­\n",
    "# tokenized_sequence = tokenizer.tokenize(sequence)  # è°ƒç”¨tokenizeæ–¹æ³•è¿›è¡Œåˆ†è¯\n",
    "# print(tokenized_sequence)  # è¾“å‡º\n",
    " \n",
    "inputs = tokenizer(sequence)\n",
    "pprint(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ef9b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4307693f80421aba98d21472f6e053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(model_name,cache_dir=cache_dir)  # æ³¨æ„è¿™é‡ŒåŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹åå­—è¦ä¸ä¸Šé¢ä¸€è‡´,éœ€è¦ä¸‹è½½æ¨¡å‹400M\n",
    "# model = BertModel.from_pretrained('./pre_model')  \n",
    "pprint(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37bfb547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3198,  0.1917,  0.1211,  ..., -0.2921,  0.3166,  0.1613],\n",
      "         [ 0.0670,  0.2243,  0.3978,  ...,  0.3485,  0.4528,  0.3749],\n",
      "         [-0.1855,  0.1676,  0.1367,  ...,  0.1331,  0.5411,  0.3667],\n",
      "         ...,\n",
      "         [ 0.2303,  0.4273,  0.2021,  ...,  0.0221, -0.2087, -0.4155],\n",
      "         [ 0.1189,  0.1697, -0.3230,  ...,  0.0310,  0.2393,  0.4445],\n",
      "         [ 0.1945,  1.2589,  0.0475,  ..., -0.2455,  0.6241, -0.0325]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "torch.Size([1, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer([sequence],return_tensors='pt')\n",
    "out = model(**inputs)\n",
    "pprint(out['last_hidden_state'])\n",
    "print(out['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e0504dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['è‹¹æœ', 'æ˜¯', 'å¾ˆ', 'å¥½', 'çš„', 'æ‰‹æœº']\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 5741, 3362, 3221, 2523, 1962, 4638, 2797, 3322,  102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./BertModel/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1031, -0.5127, -0.2884,  ...,  0.3421, -0.2324, -0.2472],\n",
      "         [ 0.4578, -0.5320, -0.1757,  ...,  0.0289, -0.7234,  0.2280],\n",
      "         [ 0.2478, -1.0135, -0.5482,  ...,  0.7351, -0.0704, -0.3028],\n",
      "         ...,\n",
      "         [ 0.0536, -1.2685, -0.0756,  ...,  0.6893,  0.6753,  0.2248],\n",
      "         [-0.0352, -0.4156, -0.8281,  ...,  1.0947,  0.1474, -0.3885],\n",
      "         [ 0.0593, -1.0608, -0.1381,  ...,  0.4914, -0.8540, -0.4325]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "torch.Size([1, 10, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from pprint import pprint\n",
    " \n",
    "vocab_file = './BertModel/bert-base-chinese-vocab.txt'\n",
    "# ä½¿ç”¨cache_dir å¯ä»¥æŒ‡å®šæ–‡ä»¶ä¸‹è½½ä½ç½®\n",
    "sequence = \"è‹¹æœ æ˜¯ å¾ˆå¥½ çš„ æ‰‹æœº\"  # å¾…åˆ†è¯çš„å¥å­\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)  # è°ƒç”¨tokenizeæ–¹æ³•è¿›è¡Œåˆ†è¯\n",
    "\n",
    "tokenized_sequence = [\"è‹¹æœ\",'æ˜¯', 'å¾ˆ', 'å¥½', 'çš„', 'æ‰‹æœº']\n",
    "print(tokenized_sequence)  # è¾“å‡º\n",
    "inputs = tokenizer([sequence],return_tensors='pt')\n",
    "encoded_sequence = inputs[\"input_ids\"]\n",
    "pprint(inputs)\n",
    " \n",
    "from transformers import BertModel\n",
    "#model = BertModel.from_pretrained(model_name,cache_dir=cache_dir)  # æ³¨æ„è¿™é‡ŒåŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹åå­—è¦ä¸ä¸Šé¢ä¸€è‡´,éœ€è¦ä¸‹è½½æ¨¡å‹400M\n",
    "model = BertModel.from_pretrained('./BertModel/bert-base-chinese')\n",
    "out = model(**inputs)\n",
    "pprint(out['last_hidden_state'])\n",
    "print(out['last_hidden_state'].shape)\n",
    "print(out['last_hidden_state'][0][1].shape)\n",
    "\n",
    "def word_to_vec(word_id, tokenized_sequence):\n",
    "    from transformers import BertTokenizer,BertModel\n",
    "    from pprint import pprint\n",
    "    \n",
    "    print(tokenized_sequence)  # è¾“å‡º\n",
    "    inputs = tokenizer([sequence],return_tensors='pt')\n",
    "    encoded_sequence = inputs[\"input_ids\"]\n",
    "    pprint(inputs)\n",
    "\n",
    "    from transformers import BertModel\n",
    "    #model = BertModel.from_pretrained(model_name,cache_dir=cache_dir)  # æ³¨æ„è¿™é‡ŒåŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹åå­—è¦ä¸ä¸Šé¢ä¸€è‡´,éœ€è¦ä¸‹è½½æ¨¡å‹400M\n",
    "    model = BertModel.from_pretrained('./BertModel/bert-base-chinese')\n",
    "    out = model(**inputs)\n",
    "#     pprint(out['last_hidden_state'])\n",
    "#     print(out['last_hidden_state'].shape)\n",
    "    print(out['last_hidden_state'][0][1].shape)\n",
    "    return out['last_hidden_state'][0][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19c6d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['è‹¹æœ', 'æ˜¯', 'å¾ˆå¥½', 'çš„', 'æ‰‹æœº']\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 5741, 3362, 3221, 2523, 1962, 4638, 2797, 3322,  102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./BertModel/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "doc1 = 'è‹¹æœ æ˜¯ å¾ˆå¥½ çš„ æ‰‹æœº'\n",
    "vec1 = word_to_vec(1, doc1.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = 'è‹¹æœ æ˜¯ å¾ˆå¥½ çš„ æ‰‹æœº'\n",
    "vec1 = word_to_vec(1, doc1.split(\" \"))\n",
    "\n",
    "doc2 = 'å¤š åƒ è‹¹æœ æœ‰ç›Š å¥åº· æ˜¯ å¾ˆå¥½ çš„ æ°´æœ'\n",
    "vec2 = word_to_vec(3, doc2.split(\" \"))\n",
    "\n",
    "str3 = 'è‹¹æœå…¬å¸1980å¹´12æœˆ12æ—¥å…¬å¼€æ‹›è‚¡ä¸Šå¸‚ï¼Œ2012å¹´åˆ›ä¸‹6235äº¿ç¾å…ƒçš„å¸‚å€¼è®°å½•ï¼Œæˆªè‡³2014å¹´6æœˆï¼Œè‹¹æœå…¬å¸å·²ç»è¿ç»­ä¸‰å¹´æˆä¸ºå…¨çƒå¸‚å€¼æœ€å¤§å…¬å¸ã€‚å½“åœ°æ—¶é—´2020å¹´8æœˆ19æ—¥ï¼Œè‹¹æœå…¬å¸å¸‚å€¼é¦–æ¬¡çªç ´2ä¸‡äº¿ç¾å…ƒã€‚ [1]  è‹¹æœå…¬å¸åœ¨2016å¹´ä¸–ç•Œ500å¼ºæ’è¡Œæ¦œä¸­æ’åç¬¬9åã€‚ [2]  2013å¹´9æœˆ30æ—¥ï¼Œåœ¨å®ç›Ÿé›†å›¢çš„â€œå…¨çƒæœ€ä½³å“ç‰Œâ€æŠ¥å‘Šä¸­ï¼Œè‹¹æœå…¬å¸è¶…è¿‡å¯å£å¯ä¹æˆä¸ºä¸–ç•Œæœ€æœ‰ä»·å€¼å“ç‰Œã€‚2014å¹´ï¼Œè‹¹æœå“ç‰Œè¶…è¶Šè°·æ­Œï¼ˆGoogleï¼‰ï¼Œæˆä¸ºä¸–ç•Œæœ€å…·ä»·å€¼å“ç‰Œã€‚2021å¹´ã€Šè´¢å¯Œã€‹ä¸–ç•Œ500å¼ºæ’è¡Œæ¦œç¬¬6åã€‚ [125]  åŒ—äº¬æ—¶é—´2022å¹´1æœˆ4æ—¥å‡Œæ™¨2ç‚¹45åˆ†å·¦å³ï¼Œç¾å›½ç§‘æŠ€å·¨å¤´è‹¹æœçš„è‚¡ä»·è¾¾åˆ°äº†182.88ç¾å…ƒï¼Œå¸‚å€¼ç¬¬ä¸€æ¬¡ç«™ä¸Šäº†ä¸‰ä¸‡äº¿ç¾å…ƒçš„å°é˜¶ï¼Œè¿™ä¸ä»…æ˜¯å…¨çƒé¦–ä¸ª3ä¸‡äº¿å¸‚å€¼ï¼Œä¹Ÿç›¸å½“äºå…¨çƒç¬¬äº”å¤§ç»æµä½“çš„GDPä½“é‡ï¼Œä»…æ¬¡äºç¾å›½ã€ä¸­å›½ã€æ—¥æœ¬åŠå¾·å›½'\n",
    "doc3 = cut_doc(str3)\n",
    "print(doc3)\n",
    "vec3 = word_to_vec(1, doc2.split(\" \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
