{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2324d1",
   "metadata": {},
   "source": [
    "# python 多线程问题\n",
    "## 列表， 字典是线程安全的\n",
    "## dataframe 不是线程安全的\n",
    "### sleep 0.1 秒 0.01秒 OK 再小会卡住\n",
    "### 线程数大概1E6会非常吃内存 2G, 危险"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85b2bf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多线程占据的全部时间12.603969812393188\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dosomthing(i):\n",
    "    time.sleep(3)\n",
    "    #print(i, \" is ok\")\n",
    "  \n",
    "    #######\n",
    "    \n",
    "    return i\n",
    "    \n",
    "with ThreadPoolExecutor(max_workers= int(1E5) ) as t:\n",
    "    obj_list = []\n",
    "    begin = time.time()\n",
    "    for i in range(0, int(1E5) ):\n",
    "        obj = t.submit(dosomthing, i)\n",
    "        obj_list.append(obj)\n",
    "    \n",
    "    for future in as_completed(obj_list):\n",
    "        data = future.result()\n",
    "        # print(data)\n",
    "    times = time.time() - begin\n",
    "    print(\"多线程占据的全部时间\"+str(times))\n",
    " \n",
    " ################# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6341bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试线程安全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23b51c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多线程占据的全部时间6.299389362335205\n",
      "10000\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ok21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ok22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ok20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ok18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ok24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>ok9981</td>\n",
       "      <td>9981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>ok9974</td>\n",
       "      <td>9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>ok9973</td>\n",
       "      <td>9973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9902</th>\n",
       "      <td>ok9902</td>\n",
       "      <td>9902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>ok9894</td>\n",
       "      <td>9894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text  label\n",
       "21      ok21     21\n",
       "22      ok22     22\n",
       "20      ok20     20\n",
       "18      ok18     18\n",
       "24      ok24     24\n",
       "...      ...    ...\n",
       "9981  ok9981   9981\n",
       "9974  ok9974   9974\n",
       "9973  ok9973   9973\n",
       "9902  ok9902   9902\n",
       "9894  ok9894   9894\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 列表是线程安全的\n",
    "thread_list = []\n",
    "# dataframe不是线程安全的\n",
    "my_df = pd.DataFrame([],index=[],columns=['text','label'])\n",
    "\n",
    "# \n",
    "map = {}\n",
    "text_map = {}\n",
    "label_map = {}\n",
    "def dosomthing(i):\n",
    "    time.sleep(4)\n",
    "    # print(i, \" is ok\")\n",
    "    thread_list.append(str(i)+\"               ok\")\n",
    "    text_map[i] = \"ok\" + str(i)\n",
    "    label_map[i] = i\n",
    "    \n",
    "    return i\n",
    "    \n",
    "with ThreadPoolExecutor(max_workers= int(1E4)) as t:\n",
    "    obj_list = []\n",
    "    begin = time.time()\n",
    "    for i in range(0, int(1E4)) :\n",
    "        obj = t.submit(dosomthing, i)\n",
    "        obj_list.append(obj)\n",
    "    \n",
    "    for future in as_completed(obj_list):\n",
    "        data = future.result()\n",
    "        # print(data)\n",
    "    times = time.time() - begin\n",
    "    print(\"多线程占据的全部时间\"+str(times))\n",
    "    print(len(thread_list))\n",
    "    print(len(map))\n",
    "\n",
    "map['text'] = text_map\n",
    "map['label'] = label_map\n",
    "\n",
    "frame3 = pd.DataFrame(map)\n",
    "frame3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "402c164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3908\n",
      "['的', '。', '是', ' ', '\\n', '日', '月', '.', '%', '\\u3000', '--', '?', '“', '”', '》', '－－', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', \"a's\", 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'控制 训练 文章 数量 '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import thulac\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "# 2. 读取停词表\n",
    "def load_stop_word():\n",
    "    stop_word = ['的', '。', '是', ' ', '\\n', '日', '月', '.', '%','\\u3000']\n",
    "    stop_word_file = 'c:/data/stopwords'\n",
    "    stop_word_dirs = os.listdir(stop_word_file)\n",
    "    for textName in stop_word_dirs:\n",
    "        filename = stop_word_file + '/' + textName\n",
    "        with open(filename, 'r', encoding='utf-8') as file_object:\n",
    "            line = file_object.readline()\n",
    "            while line:\n",
    "                stop_word.append(line[0:-1])\n",
    "                line = file_object.readline()\n",
    "    return stop_word\n",
    "\n",
    "stop_word = load_stop_word()\n",
    "print(len(stop_word))\n",
    "print(stop_word[0:100])\n",
    "\n",
    "\n",
    "# 2.5 分词前修改thulac源码中的bug\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def add_clock_method_to_time():\n",
    "    py_gt_3_8 = not hasattr(time, \"clock\")\n",
    "    if py_gt_3_8:\n",
    "        setattr(time, \"clock\", time.perf_counter)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if py_gt_3_8:\n",
    "            delattr(time, \"clock\")\n",
    "            \n",
    "#分词\n",
    " # 默认模式，分词的同时进行词性标注\n",
    "# thu = thulac.thulac(filt=True, seg_only=True)\n",
    "word_type = {}\n",
    "def cut_doc(doc):\n",
    "     # 默认模式，分词的同时进行词性标注\n",
    "    \n",
    "    words = jieba.cut(doc)\n",
    "    words_in_doc = \"\"\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        if word in stop_word: continue\n",
    "        words_in_doc +=  word + \" \"\n",
    "        # word_type[word] = wordType\n",
    "        \n",
    "    return words_in_doc\n",
    "\n",
    "words_in_doc = cut_doc(\"用来控制训练的文章数量\")\n",
    "words_in_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b045ba",
   "metadata": {},
   "source": [
    "# 多线程读取原生 的 数据集2G  每个类别读3000个新闻\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "59c7a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多线程占据的全部时间503.9964108467102\n",
      "27986\n",
      "27986\n",
      "27986\n",
      "错误的次数 0\n",
      "(27986, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_map = {}\n",
    "label_map = {}\n",
    "map = {}\n",
    "DOC_COUNT_MAX = 2000\n",
    "wrong_num = []\n",
    "\n",
    "def read_file_cut(doc_file_name, doc_id, label_index):\n",
    "    with open(doc_file_name, 'r', encoding='utf-8') as file_object:\n",
    "        doc_content = \"\"\n",
    "        line = file_object.readline()\n",
    "        while line:\n",
    "            doc_content += line\n",
    "            line = file_object.readline()\n",
    "        wrong = 1\n",
    "        for _ in range(0,4):\n",
    "            try:\n",
    "                doc_map[doc_id] = cut_doc(doc_content)\n",
    "                wrong = 0\n",
    "                break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        if wrong == 1:\n",
    "            wrong_num.append(doc_id)\n",
    "        label_map[doc_id] = int(label_index)\n",
    "    return 0\n",
    "\n",
    "my_df = pd.DataFrame([],index=[],columns=['text','label'])\n",
    "\n",
    "label_num = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers= int(10000)) as t:\n",
    "    obj_list = []\n",
    "    begin = time.time()\n",
    "    \n",
    "    news_file = 'c:/data/THUCNews'\n",
    "    news_file_dirs = os.listdir(news_file)\n",
    "    # print(news_file_dirs)\n",
    "    \n",
    "    label_index = -1\n",
    "    for label in news_file_dirs:\n",
    "        label_index += 1\n",
    "        label_num[label] = label_index\n",
    "        \n",
    "        file = news_file + '/' + label\n",
    "        doc_ids = os.listdir(file)\n",
    "        doc_num = 0\n",
    "        for doc_name in doc_ids:\n",
    "            doc_num += 1\n",
    "            if doc_num >= DOC_COUNT_MAX:\n",
    "                break\n",
    "                \n",
    "            doc_file_name = file + '/' + doc_name\n",
    "            doc_id = doc_name[0:-4]\n",
    "            doc_content = \"\"\n",
    "            \n",
    "            \n",
    "            obj = t.submit(read_file_cut, doc_file_name, doc_id, label_index)\n",
    "            obj_list.append(obj)    \n",
    "            \n",
    "      \n",
    "    for future in as_completed(obj_list):\n",
    "        data = future.result()\n",
    "    times = time.time() - begin\n",
    "    print(\"多线程占据的全部时间\"+str(times))\n",
    "    print(len(obj_list))\n",
    "    print(len(doc_map))\n",
    "    print(len(label_map))\n",
    "    \n",
    "map['text'] = doc_map\n",
    "map['label'] = label_map\n",
    "\n",
    "\n",
    "my_df = pd.DataFrame(map)\n",
    "train_df = my_df\n",
    "print(\"错误的次数\", len(wrong_num) )\n",
    "print(train_df.shape)\n",
    "train_df.to_csv(\"c:/data/news/train_data1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d16d97f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  label\n",
      "0       马晓旭 意外 受伤 国奥 警惕 无奈 大雨 青睐 殷家 军 记者 傅亚雨 沈阳 报道 来到 ...      0\n",
      "1       商瑞华 首战 复仇 心切 中国 玫瑰 美国 方式 攻克 瑞典 多曼来 瑞典 商瑞华 首战 求...      0\n",
      "100     辽足 签约 危机 引 注册 难关 高层 威逼利诱 合同 笑里藏刀 新浪 体育讯 24 辽足 ...      0\n",
      "10000   阿的江 八一 需 定位 机会 进 没进 新浪 体育讯 12 24 回到 主场 北京 金隅 迎...      0\n",
      "1000    揭秘 谢亚龙 带走 总局 电话 骗局 复制 南杨 轨迹 体坛周报 特约记者 张锐 北京 报道...      0\n",
      "...                                                   ...    ...\n",
      "800787  全球 藏家 吹 印度 风 中国 知名 藏家 张锐 杨斌 出手 印度 当代艺术 走进 中国 理...     13\n",
      "800746  贵 国庆 邮票 已翻 万倍 贵金属 藏品 交易 火爆 邮票 最贵 翻 万倍 最平刚 面值 记...     13\n",
      "800884  还原 购物 天堂 迪拜 真实 面貌 离天 建筑 鸟瞰 迪拜 迪拜 阿拉伯 世界 一颗 耀眼 ...     13\n",
      "800612  傅抱石 艺术 作品 行情 走向 步入 21 世纪 傅抱石 作品 异军突起 捷报频传 一本 毛...     13\n",
      "800835  烟雾 袅袅 圈子 雪茄 赏 最低 调 奢华 策划 编辑 宋慧敏     文 紫檀      ...     13\n",
      "\n",
      "[27986 rows x 2 columns]\n",
      "0          0\n",
      "1          0\n",
      "100        0\n",
      "10000      0\n",
      "1000       0\n",
      "          ..\n",
      "800787    13\n",
      "800746    13\n",
      "800884    13\n",
      "800612    13\n",
      "800835    13\n",
      "Name: label, Length: 27986, dtype: int32\n",
      "(1, 3000)\n",
      "[10  2  0  9  2  8  3  2  6  0  9 12 11  2  5  2  8 12 13  0  2  2  8 12\n",
      "  5  8 10  3  1  2  2  9  8  2 10  6 10 11 12  2  8  8  2  2  0  5  6  6\n",
      "  8  9 13  6 13  2  2  6  2 10  8  2  3  4  2  8  0 13  1  2  0  1  0  6\n",
      "  3  7  8  8 11  2  2  9]\n",
      "0.6750626493094043\n"
     ]
    }
   ],
   "source": [
    "# Count Vectors + RidgeClassifier\n",
    "print(train_df)\n",
    "print(train_df['label'])\n",
    "train_df['label'] = train_df['label'].astype('int')\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_df = train_df.sample(frac = 1) #打乱样本\n",
    "\n",
    "all_data_size = 1200\n",
    "train_data_size = 1000\n",
    "test_data_size = all_data_size - train_data_size\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "train_test = vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "print(train_test[0].shape )\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:train_data_size], train_df['label'].values[:train_data_size])\n",
    "val_pred = clf.predict(train_test[train_data_size:])\n",
    "print(f1_score(train_df['label'].values[train_data_size:], val_pred, average='macro'))\n",
    "# 0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "379609aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8033556405276042\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF +  RidgeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,4), max_features=300)\n",
    "\n",
    "train_test = tfidf.fit_transform(train_df['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:train_data_size], train_df['label'].values[:train_data_size])\n",
    "\n",
    "val_pred = clf.predict(train_test[train_data_size:])\n",
    "\n",
    "print(f1_score(train_df['label'].values[train_data_size:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c8fb81db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9137776175599605\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "useless = set()\n",
    "\n",
    "doc_data = train_df['text']\n",
    "common_text = []\n",
    "for i in range(0, doc_data.size):\n",
    "    doc = str(doc_data[i]).split(\" \")\n",
    "    common_text.append(doc)\n",
    "\n",
    "# 词向量的维度，样本够的话300-500。\n",
    "# min_count 最小次数计数，出现次数低于这个数的就不要了\n",
    "# workers  线程数\n",
    "# window 窗口大小\n",
    "# sample 负例采样设置aa\n",
    "\n",
    "\n",
    "vector_size = 500\n",
    "model = Word2Vec(sentences=common_text, vector_size=vector_size, window=5, min_count=4,\n",
    "                  workers=12, sample=1e-5)\n",
    "# model.save(\"word2vec.model\")\n",
    "vector = model.wv['中国']\n",
    "\n",
    "# 词向量 合成 文本向量\n",
    "doc_v_list = np.array([])\n",
    "\n",
    "for split_doc in common_text:\n",
    "    used_word = float(0.0)\n",
    "    vector = vector * 0\n",
    "    for word in split_doc:\n",
    "\n",
    "        used_word += 1.0\n",
    "        try:\n",
    "            vector += model.wv[word]\n",
    "        except Exception as e:\n",
    "            # print(e.__traceback__.tb_frame.f_globals[\"__file__\"])  # 发生异常所在的文件\n",
    "            # print(e.__traceback__.tb_lineno)  # 发生异常所在的行数\n",
    "            used_word -= 1.0\n",
    "\n",
    "    vector /= used_word /200\n",
    "    doc_v_list = np.append(doc_v_list, vector)\n",
    "\n",
    "train_test = doc_v_list.reshape(len(common_text), vector_size)\n",
    "\n",
    "# 使用word2vec \n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:train_data_size], train_df['label'].values[:train_data_size])\n",
    "\n",
    "val_pred = clf.predict(train_test[train_data_size:])\n",
    "print(f1_score(train_df['label'].values[train_data_size:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54451064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
