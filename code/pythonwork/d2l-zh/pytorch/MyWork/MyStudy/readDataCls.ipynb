{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989c0901",
   "metadata": {},
   "source": [
    "# 1. 加载依赖包\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c59b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thulac\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb14cd",
   "metadata": {},
   "source": [
    "# 2. 读取停词表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "544d5a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3908\n",
      "['的', '。', '是', ' ', '\\n', '日', '月', '.', '%', '\\u3000', '--', '?', '“', '”', '》', '－－', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', \"a's\", 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_stop_word():\n",
    "    stop_word = ['的', '。', '是', ' ', '\\n', '日', '月', '.', '%','\\u3000']\n",
    "    stop_word_file = 'c:/data/stopwords'\n",
    "    stop_word_dirs = os.listdir(stop_word_file)\n",
    "    for textName in stop_word_dirs:\n",
    "        filename = stop_word_file + '/' + textName\n",
    "        with open(filename, 'r', encoding='utf-8') as file_object:\n",
    "            line = file_object.readline()\n",
    "            while line:\n",
    "                stop_word.append(line[0:-1])\n",
    "                line = file_object.readline()\n",
    "    return stop_word\n",
    "\n",
    "stop_word = load_stop_word()\n",
    "print(len(stop_word))\n",
    "print(stop_word[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c0ca1",
   "metadata": {},
   "source": [
    "# 2.5 分词前修改thulac源码中的bug\n",
    "\n",
    "对thulac的2个bug进行修改\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fb64068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def add_clock_method_to_time():\n",
    "    py_gt_3_8 = not hasattr(time, \"clock\")\n",
    "    if py_gt_3_8:\n",
    "        setattr(time, \"clock\", time.perf_counter)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if py_gt_3_8:\n",
    "            delattr(time, \"clock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43749d2c",
   "metadata": {},
   "source": [
    "# 3. 分词\n",
    "\n",
    "输入一个文本 对文本中的词语 按照 空格 切分 去掉标点等停词\n",
    "填充一个全局的字典， 词语 -> 词性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e27fd1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'控制': 'v', '训练': 'v', '文章': 'n', '数量': 'n'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # 默认模式，分词的同时进行词性标注\n",
    "thu = thulac.thulac(filt=True, seg_only=False)\n",
    "word_type = {}\n",
    "def cut_doc(doc):\n",
    "    with add_clock_method_to_time():\n",
    "        words = thu.cut(doc)\n",
    "    words_in_doc = \"\"\n",
    "    for word, wordType in words:\n",
    "        if word in stop_word: continue\n",
    "        words_in_doc +=  word + \" \"\n",
    "        word_type[word] = wordType\n",
    "        \n",
    "    return words_in_doc\n",
    "\n",
    "words_in_doc = cut_doc(\"用来控制训练的文章数量\")\n",
    "words_in_doc\n",
    "\n",
    "word_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b39eb",
   "metadata": {},
   "source": [
    "# 4. 读取数据\n",
    "\n",
    "每类新闻中抽取1000个新闻构建 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7401d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['体育', '娱乐', '家居', '彩票', '房产', '教育', '时尚', '时政', '星座', '游戏', '社会', '科技', '股票', '财经']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>马晓旭 意外 受伤 警惕 大雨 青睐 殷 家 军 记者 傅亚雨 沈阳 报道 沈阳 摆脱 雨水...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>商瑞华 首战 复仇 心切 中国 玫瑰 美国 方式 攻克 瑞典 曼 瑞典 商瑞华 求 信心 也...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>冠军 球队 迎新 欢乐派 黄旭 获 大奖 张军 赢 PK赛 新 浪 体育 冠军 高尔夫球队 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>辽足 签约 危机 引 注册 难关 高层 威逼利诱 合同 笑里藏刀 新 浪 体育 爆发 集体 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>揭秘 谢亚龙 带走 总局 电话 骗 局 复制 杨 轨迹 体坛 周报 特约 记者 张锐 北京 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     马晓旭 意外 受伤 警惕 大雨 青睐 殷 家 军 记者 傅亚雨 沈阳 报道 沈阳 摆脱 雨水...     0\n",
       "1     商瑞华 首战 复仇 心切 中国 玫瑰 美国 方式 攻克 瑞典 曼 瑞典 商瑞华 求 信心 也...     0\n",
       "10    冠军 球队 迎新 欢乐派 黄旭 获 大奖 张军 赢 PK赛 新 浪 体育 冠军 高尔夫球队 ...     0\n",
       "100   辽足 签约 危机 引 注册 难关 高层 威逼利诱 合同 笑里藏刀 新 浪 体育 爆发 集体 ...     0\n",
       "1000  揭秘 谢亚龙 带走 总局 电话 骗 局 复制 杨 轨迹 体坛 周报 特约 记者 张锐 北京 ...     0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = pd.DataFrame([],index=[],columns=['text','label'])\n",
    "\n",
    "label_num = {}\n",
    "\n",
    "# 返回 list[str]\n",
    "def read_content():\n",
    "    # doc_data 记录 文章数据\n",
    "    \n",
    "    # my_df = pd.DataFrame([],index=[],columns=['text','label'])\n",
    "    \n",
    "    DOC_COUNT_MAX = 100\n",
    "    \n",
    "    \n",
    "    news_file = 'c:/data/THUCNews'\n",
    "    news_file_dirs = os.listdir(news_file)\n",
    "    print(news_file_dirs)\n",
    "    \n",
    "    label_index = -1\n",
    "    for label in news_file_dirs:\n",
    "        label_index += 1\n",
    "        label_num[label] = label_index\n",
    "        \n",
    "        file = news_file + '/' + label\n",
    "        doc_ids = os.listdir(file)\n",
    "        doc_num = 0\n",
    "        for doc_name in doc_ids:\n",
    "            doc_num += 1\n",
    "            if doc_num >= DOC_COUNT_MAX:\n",
    "                break\n",
    "            doc_file_name = file + '/' + doc_name\n",
    "            doc_id = doc_name[0:-4]\n",
    "            \n",
    "            doc_content = \"\"\n",
    "            \n",
    "            with open(doc_file_name, 'r', encoding='utf-8') as file_object:\n",
    "                line = file_object.readline()\n",
    "                while line:\n",
    "                    doc_content += line\n",
    "                    line = file_object.readline()\n",
    "                    \n",
    "            # print(doc_name[0:-4], len(doc_content),label)\n",
    "            my_df.loc[doc_id] = [cut_doc(doc_content), int(label_index)]\n",
    "            \n",
    "           \n",
    "    return my_df\n",
    "\n",
    "my_df = read_content()\n",
    "\n",
    "train_df = my_df\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61c75d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1386.000000\n",
      "mean      271.459596\n",
      "std       253.710538\n",
      "min         6.000000\n",
      "25%        91.000000\n",
      "50%       208.000000\n",
      "75%       367.500000\n",
      "max      2265.000000\n",
      "Name: text_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib  inline\n",
    "train_df = my_df\n",
    "train_df['text_len'] = train_df['text'].apply(lambda x: len(x.split(' ')))\n",
    "print(train_df['text_len'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4be2d885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Squares')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "_ = plt.hist(train_df['text_len'], bins=200)\n",
    "plt.xlabel('Text char count')\n",
    "plt.title(\"Histogram of char count\")\n",
    "x_values=list(range(0,1000,50))\n",
    "#x轴的数字是0到10这11个整数\n",
    "y_values=[x/10 for x in x_values]\n",
    "\n",
    "#用plot函数绘制折线图，线条颜色设置为绿色\n",
    "plt.title('Squares',fontsize=24)\n",
    "#设置图表标题和标题字号\n",
    "plt.tick_params(axis='both',which='major',labelsize=14)\n",
    "#设置刻度的字号\n",
    "plt.xlabel('Numbers',fontsize=14)\n",
    "#设置x轴标签及其字号\n",
    "plt.ylabel('Squares',fontsize=14)\n",
    "#设置y轴标签及其字号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46fdc2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41130\n",
      "('家', 5541)\n",
      "('说', 3236)\n",
      "('价', 1969)\n",
      "('刘宝瑞', 1)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "all_lines = ' '.join(list(train_df['text']))\n",
    "word_count = Counter(all_lines.split(' '))\n",
    "word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)\n",
    "\n",
    "print(len(word_count))\n",
    "\n",
    "print(word_count[0])\n",
    "print(word_count[1])\n",
    "print(word_count[2])\n",
    "\n",
    "print(word_count[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afed8b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          0\n",
      "10         0\n",
      "100        0\n",
      "1000       0\n",
      "          ..\n",
      "799071    13\n",
      "799072    13\n",
      "799073    13\n",
      "799074    13\n",
      "799075    13\n",
      "Name: label, Length: 1386, dtype: int32\n",
      "(1, 3000)\n",
      "0.012471655328798185\n",
      "0.019585253456221197\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Count Vectors + RidgeClassifier\n",
    "print(train_df['label'])\n",
    "train_df['label'] = train_df['label'].astype('int')\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "all_data_size = 1200\n",
    "train_data_size = 1000\n",
    "test_data_size = all_data_size - train_data_size\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "train_test = vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "print(train_test[0].shape )\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:train_data_size], train_df['label'].values[:train_data_size])\n",
    "val_pred = clf.predict(train_test[train_data_size:])\n",
    "\n",
    "print(f1_score(train_df['label'].values[train_data_size:], val_pred, average='macro'))\n",
    "# 0.28\n",
    "\n",
    "# svm\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=30, gamma=0.5, decision_function_shape='ovo')\n",
    "clf.fit(train_test[:train_data_size], train_df['label'].values[:train_data_size])\n",
    "val_pred = clf.predict(train_test[train_data_size:])\n",
    "\n",
    "print(f1_score(train_df['label'].values[train_data_size:], val_pred, average='macro'))\n",
    "\n",
    "\n",
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors = 13 , weights='distance')\n",
    "clf.fit(train_test[:train_data_size], train_df['label'].values[:train_data_size])\n",
    "val_pred = clf.predict(train_test[train_data_size:])\n",
    "\n",
    "print(f1_score(train_df['label'].values[train_data_size:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fa63be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017857142857142856\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF +  RidgeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,4), max_features=3000)\n",
    "\n",
    "train_test = tfidf.fit_transform(train_df['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:train_data_size], train_df['label'].values[:train_data_size])\n",
    "\n",
    "val_pred = clf.predict(train_test[train_data_size:])\n",
    "\n",
    "print(f1_score(train_df['label'].values[train_data_size:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e80eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入外部语义来训练word2vec\n",
    "# 从之前的项目中找\n",
    "import pymysql\n",
    "\n",
    "db = pymysql.connect(host=\"47.97.215.99\", user=\"root\", passwd=\"3.1415Pai\", database=\"news\", charset='utf8')\n",
    "# 使用cursor()方法获取操作游标\n",
    "cursor = db.cursor()\n",
    "\n",
    "# 读取数据库 create_table_news 中的title\n",
    "sql = \"SELECT  title \" \\\n",
    "      \"FROM create_table_news \" \\\n",
    "      \"ORDER BY news_id DESC \" \\\n",
    "      \"LIMIT 100 \"\n",
    "cursor.execute(sql)\n",
    "title_list = cursor.fetchall()\n",
    "\n",
    "# doc_data 记录 文章数据\n",
    "doc_data = set()\n",
    "\n",
    "for doc in title_list:\n",
    "    if doc == None or len(doc) < 1:\n",
    "        continue\n",
    "    doc = str(doc[0])\n",
    "    doc = doc.replace(\" \",\"\").replace(\"\\n\",\"\")\n",
    "    if doc == None or len(doc) < 1:\n",
    "        continue\n",
    "    doc_data.add(doc)\n",
    "    \n",
    "# 读取数据库 create_table_news 中的title\n",
    "sql1 = \"SELECT doc_content \" \\\n",
    "      \"FROM create_table_news \" \\\n",
    "      \"ORDER BY news_id DESC \" \\\n",
    "      \"LIMIT 100 \"\n",
    "cursor.execute(sql1)\n",
    "doc_content_list = cursor.fetchall()\n",
    "for doc in doc_content_list:\n",
    "    if doc == None or len(doc) < 1:\n",
    "        continue\n",
    "    doc = str(doc[0])\n",
    "    doc = doc.replace(\" \",\"\").replace(\"\\n\",\"\")\n",
    "    if doc == None or len(doc) < 1:\n",
    "        continue\n",
    "    doc_data.add(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf1761",
   "metadata": {},
   "source": [
    "# 为word2vec 准备 切割后的语料common_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83907d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "doc_data = train_df['text']\n",
    "common_text = []\n",
    "for i in range(0, doc_data.size):\n",
    "    doc = str(doc_data[i]).split(\" \")\n",
    "    common_text.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b084f",
   "metadata": {},
   "source": [
    "# 训练word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "useless = set()\n",
    "for i in range(0, 4):\n",
    "    if int(word_count[i][1]) > len(word_count)*2:\n",
    "        useless.add(word_count[i][0])\n",
    "doc_data = train_df['text']\n",
    "common_text = []\n",
    "for i in range(0, doc_data.size):\n",
    "    doc = str(doc_data[i]).split(\" \")\n",
    "    common_text.append(doc)\n",
    "\n",
    "# 词向量的维度，样本够的话300-500。\n",
    "# min_count 最小次数计数，出现次数低于这个数的就不要了\n",
    "# workers  线程数\n",
    "# window 窗口大小\n",
    "# sample 负例采样设置aa\n",
    "\n",
    "\n",
    "vector_size = 500\n",
    "model = Word2Vec(sentences=common_text, vector_size=vector_size, window=5, min_count=4,\n",
    "                  workers=12, sample=1e-5)\n",
    "# model.save(\"word2vec.model\")\n",
    "vector = model.wv['中国']\n",
    "\n",
    "# 词向量 合成 文本向量\n",
    "doc_v_list = np.array([])\n",
    "\n",
    "for split_doc in common_text:\n",
    "    used_word = float(0.0)\n",
    "    vector = vector * 0\n",
    "    for word in split_doc:\n",
    "        \n",
    "        if word in useless: continue\n",
    "\n",
    "        used_word += 1.0\n",
    "        try:\n",
    "            vector += model.wv[word]\n",
    "        except Exception as e:\n",
    "            # print(e.__traceback__.tb_frame.f_globals[\"__file__\"])  # 发生异常所在的文件\n",
    "            # print(e.__traceback__.tb_lineno)  # 发生异常所在的行数\n",
    "            used_word -= 1.0\n",
    "\n",
    "    vector /= used_word /200\n",
    "    doc_v_list = np.append(doc_v_list, vector)\n",
    "\n",
    "train_test = doc_v_list.reshape(len(common_text), vector_size)\n",
    "\n",
    "# 使用word2vec \n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:train_data_size], train_df['label'].values[:train_data_size])\n",
    "\n",
    "val_pred = clf.predict(train_test[train_data_size:])\n",
    "print(f1_score(train_df['label'].values[train_data_size:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f254d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7cb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
