{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6709ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thulac\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7dcb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3908\n",
      "['的', '。', '是', ' ', '\\n', '日', '月', '.', '%', '\\u3000', '--', '?', '“', '”', '》', '－－', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', \"a's\", 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain']\n"
     ]
    }
   ],
   "source": [
    "# 停词表\n",
    "\n",
    "def load_stop_word():\n",
    "    stop_word = ['的', '。', '是', ' ', '\\n', '日', '月', '.', '%','\\u3000']\n",
    "    stop_word_file = 'c:/data/stopwords'\n",
    "    stop_word_dirs = os.listdir(stop_word_file)\n",
    "    for textName in stop_word_dirs:\n",
    "        filename = stop_word_file + '/' + textName\n",
    "        with open(filename, 'r', encoding='utf-8') as file_object:\n",
    "            line = file_object.readline()\n",
    "            while line:\n",
    "                stop_word.append(line[0:-1])\n",
    "                line = file_object.readline()\n",
    "    return stop_word\n",
    "\n",
    "stop_word = load_stop_word()\n",
    "print(len(stop_word))\n",
    "print(stop_word[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8bdbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\hys\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3908\n",
      "['的', '。', '是', ' ', '\\n', '日', '月', '.', '%', '\\u3000', '--', '?', '“', '”', '》', '－－', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', \"a's\", 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.529 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'控制 训练 文章 数量 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word = load_stop_word()\n",
    "print(len(stop_word))\n",
    "print(stop_word[0:100])\n",
    "\n",
    "\n",
    "# 2.5 分词前修改thulac源码中的bug\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def add_clock_method_to_time():\n",
    "    py_gt_3_8 = not hasattr(time, \"clock\")\n",
    "    if py_gt_3_8:\n",
    "        setattr(time, \"clock\", time.perf_counter)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if py_gt_3_8:\n",
    "            delattr(time, \"clock\")\n",
    "            \n",
    "#分词\n",
    " # 默认模式，分词的同时进行词性标注\n",
    "# thu = thulac.thulac(filt=True, seg_only=True)\n",
    "word_type = {}\n",
    "def cut_doc(doc):\n",
    "     # 默认模式，分词的同时进行词性标注\n",
    "    \n",
    "    words = jieba.cut(doc)\n",
    "    words_in_doc = \"\"\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        if word in stop_word: continue\n",
    "        words_in_doc +=  word + \" \"\n",
    "        # word_type[word] = wordType\n",
    "        \n",
    "    return words_in_doc\n",
    "\n",
    "words_in_doc = cut_doc(\"用来控制训练的文章数量\")\n",
    "words_in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3876bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多线程占据的全部时间73.79589986801147\n",
      "2786\n",
      "2786\n",
      "2786\n",
      "错误的次数 0\n",
      "(2786, 2)\n"
     ]
    }
   ],
   "source": [
    "doc_map = {}\n",
    "label_map = {}\n",
    "map = {}\n",
    "DOC_COUNT_MAX = 200\n",
    "wrong_num = []\n",
    "\n",
    "def read_file_cut(doc_file_name, doc_id, label_index):\n",
    "    with open(doc_file_name, 'r', encoding='utf-8') as file_object:\n",
    "        doc_content = \"\"\n",
    "        line = file_object.readline()\n",
    "        while line:\n",
    "            doc_content += line\n",
    "            line = file_object.readline()\n",
    "        wrong = 1\n",
    "        for _ in range(0,4):\n",
    "            try:\n",
    "                doc_map[doc_id] = cut_doc(doc_content)\n",
    "                wrong = 0\n",
    "                break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        if wrong == 1:\n",
    "            wrong_num.append(doc_id)\n",
    "        label_map[doc_id] = int(label_index)\n",
    "    return 0\n",
    "\n",
    "my_df = pd.DataFrame([],index=[],columns=['text','label'])\n",
    "\n",
    "label_num = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers= int(10000)) as t:\n",
    "    obj_list = []\n",
    "    begin = time.time()\n",
    "    \n",
    "    news_file = 'c:/data/THUCNews'\n",
    "    news_file_dirs = os.listdir(news_file)\n",
    "    # print(news_file_dirs)\n",
    "    \n",
    "    label_index = -1\n",
    "    for label in news_file_dirs:\n",
    "        label_index += 1\n",
    "        label_num[label] = label_index\n",
    "        \n",
    "        file = news_file + '/' + label\n",
    "        doc_ids = os.listdir(file)\n",
    "        doc_num = 0\n",
    "        for doc_name in doc_ids:\n",
    "            doc_num += 1\n",
    "            if doc_num >= DOC_COUNT_MAX:\n",
    "                break\n",
    "                \n",
    "            doc_file_name = file + '/' + doc_name\n",
    "            doc_id = doc_name[0:-4]\n",
    "            doc_content = \"\"\n",
    "            \n",
    "            \n",
    "            obj = t.submit(read_file_cut, doc_file_name, doc_id, label_index)\n",
    "            obj_list.append(obj)    \n",
    "            \n",
    "      \n",
    "    for future in as_completed(obj_list):\n",
    "        data = future.result()\n",
    "    times = time.time() - begin\n",
    "    print(\"多线程占据的全部时间\"+str(times))\n",
    "    print(len(obj_list))\n",
    "    print(len(doc_map))\n",
    "    print(len(label_map))\n",
    "    \n",
    "map['text'] = doc_map\n",
    "map['label'] = label_map\n",
    "\n",
    "\n",
    "my_df = pd.DataFrame(map)\n",
    "train_df = my_df\n",
    "print(\"错误的次数\", len(wrong_num) )\n",
    "print(train_df.shape)\n",
    "train_df.to_csv(\"c:/data/news/train_data301.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d30d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "useless = set()\n",
    "\n",
    "doc_data = train_df['text']\n",
    "common_text = []\n",
    "for i in range(0, doc_data.size):\n",
    "    doc = str(doc_data[i]).split(\" \")\n",
    "    common_text.append(doc)\n",
    "\n",
    "# 词向量的维度，样本够的话300-500。\n",
    "# min_count 最小次数计数，出现次数低于这个数的就不要了\n",
    "# workers  线程数\n",
    "# window 窗口大小\n",
    "# sample 负例采样设置aa\n",
    "\n",
    "\n",
    "vector_size = 500\n",
    "model = Word2Vec(sentences=common_text, vector_size=vector_size, window=5, min_count=4,\n",
    "                  workers=12, sample=1e-5)\n",
    "# model.save(\"word2vec.model\")\n",
    "vector = model.wv['中国']\n",
    "\n",
    "# 词向量 合成 文本向量\n",
    "doc_v_list = np.array([])\n",
    "\n",
    "for split_doc in common_text:\n",
    "    used_word = float(0.0)\n",
    "    vector = vector * 0\n",
    "    for word in split_doc:\n",
    "\n",
    "        used_word += 1.0\n",
    "        try:\n",
    "            vector += model.wv[word]\n",
    "        except Exception as e:\n",
    "            # print(e.__traceback__.tb_frame.f_globals[\"__file__\"])  # 发生异常所在的文件\n",
    "            # print(e.__traceback__.tb_lineno)  # 发生异常所在的行数\n",
    "            used_word -= 1.0\n",
    "\n",
    "    vector /= used_word /200\n",
    "    doc_v_list = np.append(doc_v_list, vector)\n",
    "\n",
    "train_test = doc_v_list.reshape(len(common_text), vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "542372e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['训练', '沈阳', '国奥队', '大雨', '中', '球员']\n",
      "175 青睐\n",
      "176 不解\n",
      "174 雨水\n",
      "173 球员\n",
      "166 大雨\n",
      "172 国奥\n",
      "*************\n",
      "['姚明', '罗萨斯', '场', '球队', '火箭', '说']\n",
      "177 之上\n",
      "178 肥仔\n",
      "176 轨道\n",
      "168 时机\n",
      "166 情况\n",
      "175 正确\n",
      "*************\n",
      "['易', '建', '联', '尼克斯', '中', '球馆']\n",
      "215 麦迪逊\n",
      "227 机会\n",
      "229 方生\n",
      "225 易建联\n",
      "224 体验\n",
      "228 春水\n",
      "*************\n",
      "['姚', '说', '昨天', '上', '球队', '德尔曼']\n",
      "180 火山\n",
      "176 取胜\n",
      "177 休斯敦\n",
      "172 球队\n",
      "179 时报\n",
      "178 华夏\n",
      "*************\n",
      "['科', '湖', '珀森', '球员', '上', '人']\n",
      "119 不错\n",
      "100 乐于\n",
      "117 球员\n",
      "63 一员\n",
      "78 坐在\n",
      "87 工作\n",
      "*************\n",
      "['李', '分', '机会', '赛', '火箭', '说']\n",
      "106 马刺\n",
      "127 华夏\n",
      "120 以往\n",
      "129 火山\n",
      "96 表现\n",
      "122 担纲\n",
      "*************\n",
      "['普里', '比拉', '兹', '队', '开拓者', '复']\n",
      "190 奥登\n",
      "185 开拓者\n",
      "194 一支\n",
      "162 感觉\n",
      "196 一名\n",
      "192 比拉\n",
      "*************\n",
      "['詹姆斯', '训练', '中', '右腿', '疼痛', '比赛']\n",
      "93 能量\n",
      "94 小林\n",
      "92 释放\n",
      "80 一堂\n",
      "90 100%\n",
      "91 比赛\n",
      "*************\n",
      "['热火', '主场', '篮网', '出', '墨', '菲']\n",
      "60 奥特\n",
      "59 莫罗\n",
      "57 篮网\n",
      "58 哈里斯\n",
      "48 详细\n",
      "43 好头\n",
      "*************\n",
      "['辽足', '俱乐部', '球员', '合同', '超', '工资']\n",
      "323 态度\n",
      "322 坦诚\n",
      "321 俱乐部\n",
      "320 辽足\n",
      "0 辽足\n",
      "319 球员\n",
      "*************\n",
      "['菲', '韩', '男篮', '新', '写真', '中']\n",
      "108 天下\n",
      "107 红遍\n",
      "57 真的\n",
      "106 写真\n",
      "104 性感\n",
      "60 发自内心\n",
      "*************\n"
     ]
    }
   ],
   "source": [
    "# 基于每个文档的词语进行聚类\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "j = 0\n",
    "ClusterNum = 6\n",
    "\n",
    "for doc in train_df['text']:\n",
    "   \n",
    "\n",
    "    used_word = 0\n",
    "    word_matrix =  np.array([])\n",
    "    # print(doc)\n",
    "    index_of_word_map = {}\n",
    "    doc = str(doc)\n",
    "#     print(doc.split())\n",
    "    for word in doc.split():\n",
    "        if len(word) < 2: \n",
    "            continue\n",
    "        try:\n",
    "            word_matrix = np.append(word_matrix, model.wv[word])\n",
    "            index_of_word_map[used_word] = word\n",
    "            used_word += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "    cluster_test = word_matrix.reshape(used_word, vector_size)\n",
    "    \n",
    "    \n",
    "    ##################### SNOWNLP ##############\n",
    "    from snownlp import SnowNLP\n",
    "    s = SnowNLP(doc)\n",
    "    print(s.keywords(ClusterNum))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ################### KMEANS #####################\n",
    "    \n",
    "    \n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    k_means =  KMeans(n_clusters=ClusterNum, random_state=9)\n",
    "    y_pred = k_means.fit_predict(cluster_test)\n",
    "\n",
    "\n",
    "\n",
    "    best_key_set = set()\n",
    "    for center in k_means.cluster_centers_:\n",
    "        # print(type(center))\n",
    "\n",
    "        best_key = 0\n",
    "        min_dist = np.sqrt(np.sum((word_vector - cluster_test[0]) ** 2))\n",
    "        \n",
    "        for key in range(0, len(cluster_test)):\n",
    "            if key in best_key_set: continue\n",
    "            word_vector = cluster_test[key]\n",
    "            #print(word_vector)\n",
    "            dist = np.sqrt(np.sum((word_vector - center) ** 2))\n",
    "            if dist < min_dist:\n",
    "                #print(dist)\n",
    "                best_key = key\n",
    "\n",
    "        best_key_set.add(best_key)\n",
    "        print(best_key, index_of_word_map[best_key])\n",
    "    \n",
    "    j += 1\n",
    "    print(\"*************\")\n",
    "    if j > 10:\n",
    "        break\n",
    "    \n",
    "# word_num, wrong_word_num, used_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "539e62e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hys\\AppData\\Local\\Temp\\ipykernel_19916\\3842295331.py:15: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  keyWords = pd.Index(names)[sort]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['球队', '31', '球员'],\n",
       "       ['防守', '火箭', '姚明'],\n",
       "       ['表现', '篮板', '比赛'],\n",
       "       ['时间', '姚明', '球队'],\n",
       "       ['调整', '这一', '球员'],\n",
       "       ['比赛', '火箭', '机会'],\n",
       "       ['情况', '主场', '球队'],\n",
       "       ['100', '客场', '比赛'],\n",
       "       ['详细', '比赛', '主场'],\n",
       "       ['球队', '赛季', '球员']], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "####################### TF-IDF #############\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,4), max_features=300)\n",
    "train_test = tfidf.fit_transform(train_df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c0ba8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hys\\AppData\\Local\\Temp\\ipykernel_19916\\4121189783.py:3: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  keyWords = pd.Index(names)[sort]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['这一', '位置', '27', '29', '事情', '一位', '25', '球队', '31', '球员'],\n",
       "       ['这是', '未来', '采访', '比赛', '原因', '表现', '球队', '防守', '火箭', '姚明'],\n",
       "       ['这是', '2009', '依然', '优势', '球队', '赛季', '时间', '表现', '篮板', '比赛'],\n",
       "       ['采访', '游戏', '回答', '球员', '火箭', '记者', '11', '时间', '姚明', '球队'],\n",
       "       ['很大', '一位', '找到', '不错', '一名', '防守', '特别', '调整', '这一', '球员'],\n",
       "       ['只能', '对手', '回答', '进攻', '时间', '两个', '结束', '比赛', '火箭', '机会'],\n",
       "       ['很大', '10', '压力', '26', '赛季', '12', '11', '情况', '主场', '球队'],\n",
       "       ['感觉', '参加', '媒体', '状态', '面对', '球队', '详细', '100', '客场', '比赛'],\n",
       "       ['新浪', '北京', '11', '这是', '北京 时间', '对手', '客场', '详细', '比赛', '主场'],\n",
       "       ['几个', '情况', '20', '2008', '24', '2009', '管理', '球队', '赛季', '球员'],\n",
       "       ['希望', '关注', '方式', '16', '参加', '真的', '依然', '优势', '近期', '一年'],\n",
       "       ['冠军', '本场', '10', '12', '表现', '16', '17', '篮板', '球队', '比赛'],\n",
       "       ['接受', '北京 时间', '第一', '不错', '采访', '这是', '表现', '一点', '球员', '球队'],\n",
       "       ['时间', '最终', '只能', '机会', '表现', '球队', '选择', '比赛', '进攻', '防守'],\n",
       "       ['记者', '美国', '报道', '一位', '结束', '不错', '位置', '比赛', '代表', '希望'],\n",
       "       ['新浪', '北京', '11', '15', '北京 时间', '结束', '一年', '赛季', '消息', '球队'],\n",
       "       ['匿名', '区域', '时间', '新浪', '北京', '11', '世界', '比赛', '北京 时间', '2010'],\n",
       "       ['新浪', '11', '比赛', '感觉', '50', '球队', '对手', '火箭', '研究', '姚明'],\n",
       "       ['对手', '记者', '球员', '防守', '分析', '找到', '比赛', '中国', '过程', '美国'],\n",
       "       ['赛季', '服务', '篮板', '比赛', '13', '2008', '终于', '球队', '职业', '最终'],\n",
       "       ['防守', '表现', '一场', '数据', '球队', '篮板', '13', '找到', '本场', '比赛']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort = np.argsort(train_test.toarray(), axis = 1)[:, -10:]\n",
    "names = tfidf.get_feature_names()\n",
    "keyWords = pd.Index(names)[sort]\n",
    "keyWords[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c69e713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08378126, -0.1839826 ,  0.16146725, ...,  0.02087184,\n",
       "        -0.13922298, -0.30269699],\n",
       "       [ 0.05630706, -0.12312471,  0.10766536, ...,  0.01407618,\n",
       "        -0.09354932, -0.2032135 ],\n",
       "       [ 0.02327915, -0.05024099,  0.04506252, ...,  0.00531093,\n",
       "        -0.03750145, -0.08308181],\n",
       "       [ 0.13345828, -0.29105409,  0.25545619, ...,  0.03428826,\n",
       "        -0.22079782, -0.47747483]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "167cd59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "球员\n",
      "190\n",
      "不解\n",
      "193\n",
      "青睐\n",
      "192\n",
      "中\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b951e23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
