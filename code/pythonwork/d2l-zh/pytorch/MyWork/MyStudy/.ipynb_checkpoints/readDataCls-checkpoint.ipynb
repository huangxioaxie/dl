{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989c0901",
   "metadata": {},
   "source": [
    "# 1. 加载依赖包\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c59b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thulac\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb14cd",
   "metadata": {},
   "source": [
    "# 2. 读取停词表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "544d5a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3907\n",
      "['的', '。', '是', ' ', '\\n', '日', '月', '.', '%', '--', '?', '“', '”', '》', '－－', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', \"a's\", 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_stop_word():\n",
    "    stop_word = ['的', '。', '是', ' ', '\\n', '日', '月', '.', '%']\n",
    "    stop_word_file = 'c:/data/stopwords'\n",
    "    stop_word_dirs = os.listdir(stop_word_file)\n",
    "    for textName in stop_word_dirs:\n",
    "        filename = stop_word_file + '/' + textName\n",
    "        with open(filename, 'r', encoding='utf-8') as file_object:\n",
    "            line = file_object.readline()\n",
    "            while line:\n",
    "                stop_word.append(line[0:-1])\n",
    "                line = file_object.readline()\n",
    "    return stop_word\n",
    "\n",
    "stop_word = load_stop_word()\n",
    "print(len(stop_word))\n",
    "print(stop_word[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c0ca1",
   "metadata": {},
   "source": [
    "# 2.5 分词前修改thulac源码中的bug\n",
    "\n",
    "对thulac的2个bug进行修改\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fb64068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def add_clock_method_to_time():\n",
    "    py_gt_3_8 = not hasattr(time, \"clock\")\n",
    "    if py_gt_3_8:\n",
    "        setattr(time, \"clock\", time.perf_counter)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if py_gt_3_8:\n",
    "            delattr(time, \"clock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43749d2c",
   "metadata": {},
   "source": [
    "# 3. 分词\n",
    "\n",
    "输入一个文本 对文本中的词语 按照 空格 切分 去掉标点等停词\n",
    "填充一个全局的字典， 词语 -> 词性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e27fd1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'控制': 'v', '训练': 'v', '文章': 'n', '数量': 'n'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # 默认模式，分词的同时进行词性标注\n",
    "thu = thulac.thulac(filt=True, seg_only=False)\n",
    "word_type = {}\n",
    "def cut_doc(doc):\n",
    "    with add_clock_method_to_time():\n",
    "        words = thu.cut(doc)\n",
    "    words_in_doc = \"\"\n",
    "    for word, wordType in words:\n",
    "        if word in stop_word: continue\n",
    "        words_in_doc +=  word + \" \"\n",
    "        word_type[word] = wordType\n",
    "        \n",
    "    return words_in_doc\n",
    "\n",
    "words_in_doc = cut_doc(\"用来控制训练的文章数量\")\n",
    "words_in_doc\n",
    "\n",
    "word_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b39eb",
   "metadata": {},
   "source": [
    "# 4. 读取数据\n",
    "\n",
    "每类新闻中抽取1000个新闻构建 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7401d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['体育', '娱乐', '家居', '彩票', '房产', '教育', '时尚', '时政', '星座', '游戏', '社会', '科技', '股票', '财经']\n",
      "0.txt 99999.txt\n",
      "131604.txt 224235.txt\n",
      "224236.txt 256821.txt\n",
      "256822.txt 264409.txt\n",
      "264410.txt 284459.txt\n",
      "284460.txt 326395.txt\n",
      "326396.txt 339763.txt\n",
      "339764.txt 402849.txt\n",
      "402850.txt 406427.txt\n",
      "406428.txt 430800.txt\n",
      "430801.txt 481649.txt\n",
      "481650.txt 644578.txt\n",
      "644579.txt 798976.txt\n",
      "798977.txt 836074.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>马晓旭 意外 受伤 警惕 大雨 青睐 殷 家 军 记者 傅亚雨 沈阳 报道 沈阳 摆脱 雨水...</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>商瑞华 首战 复仇 心切 中国 玫瑰 美国 方式 攻克 瑞典 　 曼 瑞典 商瑞华 求 信心...</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>冠军 球队 迎新 欢乐派 黄旭 获 大奖 张军 赢 PK赛 　 新 浪 体育 　 冠军 高尔...</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>辽足 签约 危机 引 注册 难关 高层 威逼利诱 合同 笑里藏刀 　 新 浪 体育 　 爆发...</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>揭秘 谢亚龙 带走 总局 电话 骗 局 复制 杨 轨迹 　 体坛 周报 特约 记者 张锐 北...</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798981</th>\n",
       "      <td>亚洲 燃料油 裂解 价 差 收 窄 跨 合约 遭 大单 抛售 文华 财经 编辑 整理 何丽丽...</td>\n",
       "      <td>财经</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798982</th>\n",
       "      <td>艺术品 秋拍 上演 中国 艺术品 市场 经历 低潮 反弹 强势 上涨 北京 荣宝 拉开 拍 ...</td>\n",
       "      <td>财经</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798983</th>\n",
       "      <td>欧佩克 会议 幅度 减产 　 新华网 阿尔及尔 电 记者 郑斌 尔及利亚 新闻社 报道 石油...</td>\n",
       "      <td>财经</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798984</th>\n",
       "      <td>中国 棉花 产量 预计 减少 　 新浪 财经 下午 消息 网站 消息 中国 储备棉 管理 总...</td>\n",
       "      <td>财经</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798985</th>\n",
       "      <td>价 高 中国 续 进口 美国 大豆 　 卢进 美国 大豆 价 令 买 家 望而却步 全球 大...</td>\n",
       "      <td>财经</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text label\n",
       "0       马晓旭 意外 受伤 警惕 大雨 青睐 殷 家 军 记者 傅亚雨 沈阳 报道 沈阳 摆脱 雨水...    体育\n",
       "1       商瑞华 首战 复仇 心切 中国 玫瑰 美国 方式 攻克 瑞典 　 曼 瑞典 商瑞华 求 信心...    体育\n",
       "10      冠军 球队 迎新 欢乐派 黄旭 获 大奖 张军 赢 PK赛 　 新 浪 体育 　 冠军 高尔...    体育\n",
       "100     辽足 签约 危机 引 注册 难关 高层 威逼利诱 合同 笑里藏刀 　 新 浪 体育 　 爆发...    体育\n",
       "1000    揭秘 谢亚龙 带走 总局 电话 骗 局 复制 杨 轨迹 　 体坛 周报 特约 记者 张锐 北...    体育\n",
       "...                                                   ...   ...\n",
       "798981  亚洲 燃料油 裂解 价 差 收 窄 跨 合约 遭 大单 抛售 文华 财经 编辑 整理 何丽丽...    财经\n",
       "798982  艺术品 秋拍 上演 中国 艺术品 市场 经历 低潮 反弹 强势 上涨 北京 荣宝 拉开 拍 ...    财经\n",
       "798983  欧佩克 会议 幅度 减产 　 新华网 阿尔及尔 电 记者 郑斌 尔及利亚 新闻社 报道 石油...    财经\n",
       "798984  中国 棉花 产量 预计 减少 　 新浪 财经 下午 消息 网站 消息 中国 储备棉 管理 总...    财经\n",
       "798985  价 高 中国 续 进口 美国 大豆 　 卢进 美国 大豆 价 令 买 家 望而却步 全球 大...    财经\n",
       "\n",
       "[126 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = pd.DataFrame([],index=[],columns=['text','label'])\n",
    "\n",
    "label_num = {}\n",
    "\n",
    "# 返回 list[str]\n",
    "def read_content():\n",
    "    # doc_data 记录 文章数据\n",
    "    \n",
    "    # my_df = pd.DataFrame([],index=[],columns=['text','label'])\n",
    "    \n",
    "    DOC_COUNT_MAX = 10\n",
    "    \n",
    "    \n",
    "    news_file = 'c:/data/THUCNews'\n",
    "    news_file_dirs = os.listdir(news_file)\n",
    "    print(news_file_dirs)\n",
    "    \n",
    "    for label in news_file_dirs:\n",
    "        file = news_file + '/' + label\n",
    "        doc_ids = os.listdir(file)\n",
    "        doc_num = 0\n",
    "        for doc_name in doc_ids:\n",
    "            doc_num += 1\n",
    "            if doc_num >= DOC_COUNT_MAX:\n",
    "                break\n",
    "            doc_file_name = file + '/' + doc_name\n",
    "            doc_id = doc_name[0:-4]\n",
    "            \n",
    "            doc_content = \"\"\n",
    "            \n",
    "            with open(doc_file_name, 'r', encoding='utf-8') as file_object:\n",
    "                line = file_object.readline()\n",
    "                while line:\n",
    "                    doc_content += line\n",
    "                    line = file_object.readline()\n",
    "                    \n",
    "            # print(doc_name[0:-4], len(doc_content),label)\n",
    "            my_df.loc[doc_id] = [cut_doc(doc_content), label]\n",
    "            \n",
    "           \n",
    "    return my_df\n",
    "\n",
    "my_df = read_content()\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61c75d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     126.000000\n",
      "mean      259.325397\n",
      "std       250.947479\n",
      "min         8.000000\n",
      "25%        91.000000\n",
      "50%       184.000000\n",
      "75%       354.250000\n",
      "max      1572.000000\n",
      "Name: text_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib  inline\n",
    "train_df = my_df\n",
    "train_df['text_len'] = train_df['text'].apply(lambda x: len(x.split(' ')))\n",
    "print(train_df['text_len'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2d885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
