## 文本表示

单文本分类任务：对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类。与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。

### Word2Vec

静态 定长 分布式词向量模型

### 注意力机制

### Transformer

## 文本特征提取

### 基于监督方法的文本特征提取器

cnn rnn
 NN的好处在于能end2end实现模型的训练和测试，利用模型的非线性和众多参数来学习特征，而不需要手工提取特征。CNN善于捕捉文本中关键的局部信息，而RNN则善于捕捉文本的上下文信息（考虑语序信息），并且有一定的记忆能力。



### 关键词提取

** **

![关键词提取](C:\Users\hys\Downloads\关键词提取.png)

### 知识图谱

** **

WordNet

CN-DBpedia

http://shuyantech.com/api/cndbpedia/ment2ent?q=红楼梦



http://shuyantech.com/api/cndbpedia/avpair?q=%E5%8D%95%E8%BA%AB%E7%8B%97%EF%BC%88%E7%BD%91%E7%BB%9C%E4%BF%9A%E8%AF%AD%EF%BC%89

### 实体链接

指代消解 Coreference resolution 是自然语言处理领域的一个重要问题。



基于知识图谱的短文本特征拓展方法



BERT原理

2018年谷歌发布了基于多层transformer的双向编码语言模型BERT。Base版本BERT有768个隐藏层，共1.1亿个参数。而large版本有接近3.3亿个参数。训练BERT模型需要在16个TPUv3上运行三天左右，优化后也需要在1024个TPUv3上大约76min。即便使用当前最新的TPUv4（相当于8个英伟达A100卡，而英伟达A100售价已经高达7万人民币），也需要256个TPU v4训练1分钟。以目前的算力水平来看，训练BERT模型对大部分普通研究者来说是依旧是不现实的。



url去重

标题的词袋（bow）向量的余弦距离刻画文本相似度

正文 相似度检测， 删除冗余新闻

** **



目标新闻识别





****

## 数据集

常见的新闻分类数据集当中，我们常以新闻所在版块确定新闻所属的类别，例如常见的中文新闻分类数据集 Sogou News[60]、THUCNews[59]、英文的新闻分类数据集 AG News[61]等





** **

## 图表

数据集文本长度分布情况统计表



# 论文引用

